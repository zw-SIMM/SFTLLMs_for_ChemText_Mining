## 1. Data for Paragraph2Role

See in ```data/role```

## 2. Methods for Paragraph2Role

### Prompt Engineering ChatGPT (GPT-4, GPT-3.5-Turbo)

See in ```prompt_chatgpt_for_paragraph2role.ipynb```

### Fine-tuning ChatGPT (GPT-3.5-Turbo)

See in ```finetune_chatgpt_for_paragraph2role.ipynb```

### Full Parameter Fine-tuning Open-source Large Language Models (Llama3, Llama2, Mistral)

Training Code in ```finetune_llms_full_for_paragraph2role.py```

Inferencing Code in ```vllm_inference_full_finetuned_llms.ipynb```

### Parameter Efficient Fine-tuning (PEFT) Open-source Large Language Models (Llama3, Llama2, Mistral)

Training Code in ```finetune_llms_peft_for_paragraph2role.py```

Inferencing Code in ```vllm_inference_peft_finetuned_llms.ipynb```

### Fine-tuning Language Models (T5, Bart)

See in ```finetune_bart_or_t5_for_paragraph2role.py```

## 3. Evaluating the results of Paragraph2Role

All predictions will be saved in ```results/predictions```

Evaluating codes are in ```evaluate_Paragraph2Role.ipynb```