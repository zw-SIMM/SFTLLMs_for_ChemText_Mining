{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code of calculating metrics are from:\n",
    "# https://github.com/rxn4chemistry/paragraph2actions/blob/main/src/paragraph2actions/analysis.py\n",
    "# https://github.com/rxn4chemistry/paragraph2actions/blob/main/src/paragraph2actions/scripts/calculate_metrics.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Tuple, Sequence, List, Optional, Iterable, Any, Iterator, Callable\n",
    "import textdistance\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def all_identical(sequence: Sequence[Any]) -> bool:\n",
    "    return all(s == sequence[0] for s in sequence)\n",
    "\n",
    "\n",
    "def highlight_differences(source_sentences: List[str], translations: Sequence[List[str]]) -> None:\n",
    "    \"\"\"\n",
    "    Will highlight sentences that are translated differently by different models.\n",
    "\n",
    "    Args:\n",
    "        source_sentences: Sentences to translate (length: L)\n",
    "        translations: Multiple lists of translations, depending on the number of translation models (size: n_models x L)\n",
    "    \"\"\"\n",
    "    assert all(len(l) == len(source_sentences) for l in translations)\n",
    "\n",
    "    for i, sentence in enumerate(source_sentences):\n",
    "        sentence_translations = [t[i] for t in translations]\n",
    "\n",
    "        if not all_identical(sentence_translations):\n",
    "            print(f'Sample {i}: {sentence}')\n",
    "            for model_no, s in enumerate(sentence_translations, 1):\n",
    "                print(f'{model_no}) {s}')\n",
    "            print()\n",
    "\n",
    "\n",
    "def full_sentence_accuracy(truth: List[str], pred: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the number of exact matches.\n",
    "    \"\"\"\n",
    "    assert len(truth) == len(pred)\n",
    "\n",
    "    correct_count = sum(int(t == p) for t, p in zip(truth, pred))\n",
    "    return correct_count / len(truth)\n",
    "\n",
    "def modified_bleu(truth: List[str], pred: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the BLEU score of a translation, with a small modification in order not to penalize sentences\n",
    "    with less than 4 words.\n",
    "\n",
    "    Returns:\n",
    "        value between 0 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    references = [sentence.split() for sentence in truth]\n",
    "    candidates = [sentence.split() for sentence in pred]\n",
    "\n",
    "    # BLEU penalizes sentences with only one word. Even correct translations get a score of zero.\n",
    "    references = [r + max(0, 4 - len(r)) * [''] for r in references]\n",
    "    candidates = [c + max(0, 4 - len(c)) * [''] for c in candidates]\n",
    "\n",
    "    # references must have a larger depth because it supports multiple choices\n",
    "    refs = [[r] for r in references]\n",
    "    return corpus_bleu(refs, candidates)\n",
    "\n",
    "\n",
    "def original_bleu(truth: List[str], pred: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the BLEU score of a translation, with the original function from nltk.\n",
    "\n",
    "    Returns:\n",
    "        value between 0 and 1.\n",
    "    \"\"\"\n",
    "    references = [sentence.split() for sentence in truth]\n",
    "    candidates = [sentence.split() for sentence in pred]\n",
    "\n",
    "    # references must have a larger depth because it supports multiple choices\n",
    "    refs = [[r] for r in references]\n",
    "    return corpus_bleu(refs, candidates)\n",
    "\n",
    "\n",
    "def bleu2(truth, pred):\n",
    "    references = [sentence.split() for sentence in truth]\n",
    "    candidates = [sentence.split() for sentence in pred]\n",
    "    refs = [[r] for r in references]\n",
    "    bleu2 = corpus_bleu(refs, candidates, weights=(.5, .5))\n",
    "    return bleu2\n",
    "\n",
    "\n",
    "def levenshtein_similarity(truth: List[str], pred: List[str]) -> float:\n",
    "    assert len(truth) == len(pred)\n",
    "    scores = (textdistance.levenshtein.normalized_similarity(t, p) for t, p in zip(truth, pred))\n",
    "    return sum(scores) / len(truth)\n",
    "\n",
    "\n",
    "def partial_accuracy(truth: List[str], pred: List[str], threshold: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy from the fraction of sentences that have a similarity to the\n",
    "    ground truth higher than a given threshold.\n",
    "\n",
    "    For threshold == 1.0, this function is equivalent to full_sentence_accuracy.\n",
    "\n",
    "    Args:\n",
    "        truth: ground truth action sequences\n",
    "        pred: predicted truth action sequences\n",
    "        threshold: threshold above which to consider it as a partial match, between 0 and 1\n",
    "    \"\"\"\n",
    "    assert len(truth) == len(pred)\n",
    "    match_count = sum(\n",
    "        1 for t, p in zip(truth, pred)\n",
    "        if textdistance.levenshtein.normalized_similarity(t, p) >= threshold\n",
    "    )\n",
    "    return match_count / len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified BLEU, pr: 0.864221426194361\n",
      "BLEU-2, pr: 0.8859320523310793\n",
      "Levenshtein, pr: 0.8991550985984681\n",
      "100% accuracy, pr: 0.6903409090909091\n",
      "90% accuracy, pr: 0.78125\n",
      "75% accuracy, pr: 0.8693181818181818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Calculate metrics for predictions generated by one or several translation models\"\"\"\n",
    "\n",
    "# load predictions and ground truth\n",
    "df = pd.read_csv(\"results/predictions/finetuned_gpt3.5_hand_annotated_train_augmented_unique_5_epoch.csv\")#.fillna(\"0\")\n",
    "\n",
    "ground_truth = list(df['Actual Text'])\n",
    "prediction = list(df['Generated Text']) \n",
    "\n",
    "# evaluations\n",
    "print('Modified BLEU, pr:', modified_bleu(ground_truth, prediction))\n",
    "print('BLEU-2, pr:', bleu2(ground_truth, prediction))\n",
    "print('Levenshtein, pr:', levenshtein_similarity(ground_truth, prediction))\n",
    "print('100% accuracy, pr:', partial_accuracy(ground_truth, prediction, 1.0))\n",
    "print('90% accuracy, pr:', partial_accuracy(ground_truth, prediction, 0.9))\n",
    "print('75% accuracy, pr:', partial_accuracy(ground_truth, prediction, 0.75))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "know",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
